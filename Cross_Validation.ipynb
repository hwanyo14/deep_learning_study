{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cross_Validation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPjcm1B1pelyoDep37dYnTU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Cross Validation: 교차 검증**\n","\n","훈련세트, 검증세트, 테스트세트 등으로 샘플을 분리하느라   \n","훈련세트와 검증세트의 샘플 개수가 줄어 모델을 훈련시킬 데이터가 부족해지는 경우   \n","**교차검증**을 이용해볼 수 있다."],"metadata":{"id":"zRp7PoaxQJd-"}},{"cell_type":"code","source":["# import dependencies\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDClassifier\n","cancer = load_breast_cancer()\n","c_data = cancer.data\n","c_target = cancer.target\n","train_data_all, test_data, train_target_all, test_target = train_test_split(c_data, c_target, stratify=c_target, test_size=0.2, random_state=42)\n","train_data, val_data, train_target, val_target = train_test_split(train_data_all, train_target_all, stratify=train_target_all, test_size=0.2, random_state=42)\n","train_mean = np.mean(train_data, axis=0)\n","train_std = np.std(train_data, axis=0)\n","train_data_scaled = (train_data - train_mean) / train_std\n","val_data_scaled = (val_data - train_mean) / train_std"],"metadata":{"id":"kEwSgboaSPsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SingleLayer:\n","\n","  def __init__(self, learning_rate=0.1, l1=0, l2=0):\n","    self.w = None   # 입력데이터의 특성이 많아 가중치와 절편을 미리 초기화하지 않는다.\n","    self.b = None   # 나중에 입력데이터를 보고 특성 개수에 맞게 결정\n","    self.losses = []\n","    self.w_history = []   # 가중치를 저장할 리스트\n","    self.lr = learning_rate   # 학습률 \n","    self.val_losses = []    # 검증세트 손실을 기록할 리스트\n","    self.l1 = l1\n","    self.l2 = l2\n","\n","  def forpass(self, x):\n","    z = np.sum(self.w * x) + self.b   # x와 w는 1차원 배열이므로 np.sum을 이용해 모든 요소를 다 더한다.\n","    return z\n","\n","\n","  def backprop(self, x, err):   # 오차역전파 메서드\n","    w_grad = x * err\n","    b_grad = err\n","    return w_grad, b_grad\n","\n","\n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):  # 검증세트를 전달받을 x_val, y_val 추가\n","    self.w = np.ones(x.shape[1])  # 가중치와 절편 초기화\n","    self.b = 0\n","    self.w_history.append(self.w.copy())  # 가중치 기록 -> 넘파이 배열(w)을 추가하면 실제값이 추가되는 것이 아닌 배열을 참조하기 때문에 w값이 바뀌면 그 값을 복사하여 추가해주어야 한다.\n","    np.random.seed(42)\n","    for i in range(epochs):\n","      loss = 0\n","      indexes = np.random.permutation(np.arange(len(x))) # 샘플 개수만큼의 인덱스 섞기\n","      for i in indexes:\n","        z = self.forpass(x[i])   # 정방향 계산\n","        a = self.activation(z)  # 정방향 계산의 결과값인 z를 활성화 함수에 통과\n","        err =  -(y[i] - a)       # 활성화 함수를 거친 a값으로 오차량 계산\n","        w_grad, b_grad = self.backprop(x[i], err)  # 오차역전파\n","        w_grad += self.l1 * np.sign(self.w) + self.l2 * self.w # 그레이디언트에서 패널티 항의 미분값을 더한다.\n","        self.w -= self.lr * w_grad   # 그레이디언트 업데이트  (학습률 적용)\n","        self.b -= b_grad\n","        self.w_history.append(self.w.copy())  # 가중치 기록\n","        a = np.clip(a, 1e-10, 1-1e-10)\n","        loss += -(y[i] * np.log(a) + (1 - y[i]) * np.log(1 - a))  # 로지스틱 손실함수 -(ylog(a) - (1 - y)log(1 - a))\n","      self.losses.append(loss/len(y) + self.reg_loss())\n","      self.update_val_loss(x_val, y_val)  # 검증세트 손실을 업데이트하는 메서드 호출\n","\n","\n","  def reg_loss(self):\n","    return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)\n","\n","  \n","  def update_val_loss(self, x_val, y_val):\n","    if x_val is None:\n","      return\n","    val_loss = 0\n","    for i in range(len(x_val)):\n","      z = self.forpass(x_val[i])\n","      a = self.activation(z)\n","      a = np.clip(a, 1e-10, 1-1e-10)\n","      val_loss += -(y_val[i] * np.log(a) + (1 - y_val[i]) * np.log(1 - a))\n","    self.val_losses.append(val_loss / len(y_val) + self.reg_loss())\n","\n","\n","  def activation(self, z):  # 활성화 함수\n","    z = np.clip(z, -100, None)    # 안전한 계산을 위해 클리핑\n","    a = 1 / (1 + np.exp(-z))\n","    return a\n","\n","\n","  def predict(self, x): # 예측 함수\n","    z = [self.forpass(x_i) for x_i in x]\n","    return np.array(z) > 0  # 계단함수\n","\n","\n","  def score(self, x, y):    # 평가함수\n","    return np.mean(self.predict(x) == y)"],"metadata":{"id":"X_b1M5NBUeb6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**K-fold Cross Validation: k-폴드 교차 검증**\n","전체 데이터셋을 훈련세트와 테스트세트로 8:2 분류한 후,   \n","다시 훈련세트를 k개의 작은 덩어리(폴드)로 나눈다.   \n","여기서 총 k 번의 성능 테스트에서 1 개의 폴드를 검증에 사용하고,   \n","(k - 1) 개의 폴드를 훈련에 사용한다.\n","\n","기존에는 훈련:테스트=8:2로 나누고,   \n","훈련:검증=8:2로 나누어, 총 6:2:2의 비율로 나누었지만,   \n","k폴드 교차검증에서는 검증세트가 훈련세트에 포함되므로   \n","단순히 전체 데이터셋을 8:2로 한 번만 나눈다.\n",">따라서 한 번만 나눈 train_data_all, train_target_all을 사용한다."],"metadata":{"id":"o81B7UKxQ1GG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eco3oug6QEI6"},"outputs":[],"source":["validation_scores = []  # 각 폴드의 검증 점수를 저장하기 위한 리스트\n","                        # 이 리스트의 값을 평균하여 최종 검증 점수를 계산한다."]},{"cell_type":"code","source":["k = 10  # 폴드 개수\n","bins = len(train_data_all) // k   # 한 폴드에 들어가는 샘플 개수\n","# 이 bins 변수의 개수만큼 건너뛰며 검증 폴드와 훈련 폴드를 구분한다.\n","\n","for i in range(k):\n","  start = i * bins  # 검증폴드 샘플의 시작 인덱스\n","  end = (i + 1) * bins  # 검증폴드 샘플의 끝 인덱스\n","  val_fold = train_data_all[start:end]\n","  val_target = train_target_all[start:end]\n","\n","  train_index = list(range(0, start)) + list(range(end, len(train_data_all))) # 검증폴드 시작과 끝 범위를 뺀 나머지는 모두 훈련폴드\n","  train_fold = train_data_all[train_index]\n","  train_target = train_target_all[train_index]\n","\n","  train_mean = np.mean(train_fold, axis=0)\n","  train_std = np.std(train_fold, axis=0)\n","  train_fold_scaled = (train_fold - train_mean) / train_std\n","  val_fold_scaled = (val_fold - train_mean) / train_std\n","\n","  lyr = SingleLayer(l2=0.01)\n","  lyr.fit(train_fold_scaled, train_target, epochs=50)\n","  score = lyr.score(val_fold_scaled, val_target)\n","  validation_scores.append(score)\n","\n","print(np.mean(validation_scores))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3R_RCecSMZd","executionInfo":{"status":"ok","timestamp":1651504415763,"user_tz":-540,"elapsed":22053,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"b8a0efba-2f33-4388-cbd1-af8827d727c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9668518518518519\n"]}]},{"cell_type":"markdown","source":["여기서 중요한 점은 '폴드를 나눈 후에 표준화 전처리를 한다'라는 점이다.   \n","만약 폴드를 나누기 전에 전체 훈련세트를 전처리한다면 검증 폴드의 정보를 누설하게 되는 셈이다."],"metadata":{"id":"F9REsLObWcJo"}},{"cell_type":"markdown","source":["#**사이킷런으로 교차검증하기**\n","\n","사이킷런의 model_selection 모듈에는 교차검증을 위한 cross_validate() 함수가 있다.   \n","우리가 만든 SingleLayer 클래스와 cross_validate() 함수를 같이 사용하기 위해 SingleLayer 클래스에 여러 기능을 추가해야 하지만,   \n","그것은 책의 범위를 벗어나므로   \n","SGDClassifier 클래스와 cross_validate() 함수를 어떻게 사용하는지만 알아보자."],"metadata":{"id":"GKGr5a9nWrD7"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_validate\n","sgd = SGDClassifier(loss='log', penalty='l2', alpha=0.001, random_state=42)\n","scores = cross_validate(sgd, train_data_all, train_target_all, cv=10) # cv 파라미터에 교차 검증을 수행할 폴드의 개수를 넣는다.\n","print(np.mean(scores['test_score']))  # 'test_score'는 검증 점수"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpNTikxJU2JV","executionInfo":{"status":"ok","timestamp":1651505097950,"user_tz":-540,"elapsed":421,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"512b3aec-2e64-4fb1-96e8-314ddadc2cda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.850096618357488\n"]}]},{"cell_type":"markdown","source":["표준화 전처리 과정을 거치지 않아 교차 검증 점수가 낮다.   \n","표준화 전처리를 해보자.\n","\n","###**Pipeline 클래스 이용하기**\n","\n","검증 폴드가 전처리 단계에서 누설되지 않도록 전처리 단계와 모델 클래스를 하나로 연결해주는 pipeline 클래스를 제공한다.\n","\n"],"metadata":{"id":"lMPPZCjvZ6fB"}},{"cell_type":"code","source":["from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","pipe = make_pipeline(StandardScaler(), sgd)\n","scores = cross_validate(pipe, train_data_all, train_target_all, cv=10, return_train_score=True)\n","print(np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43Jp2iQUX-fl","executionInfo":{"status":"ok","timestamp":1651506002481,"user_tz":-540,"elapsed":424,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"c09396c0-2b1d-40e5-ec52-f9eef9ad5e70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9694202898550724\n"]}]},{"cell_type":"code","source":["print(np.mean(scores['train_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jP-tphTkbbT1","executionInfo":{"status":"ok","timestamp":1651506026549,"user_tz":-540,"elapsed":17,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"79cc1d0b-6ddd-49c6-ad2f-de179f59af69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9875478561631581\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"dRCVkJB7bhSS"},"execution_count":null,"outputs":[]}]}