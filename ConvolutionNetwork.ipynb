{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Doit_ConvolutionNetwork.ipynb","provenance":[],"collapsed_sections":["oDkB-cMJo2VF"],"authorship_tag":"ABX9TyM/9oFZZ/EGdgkuEDFWxNb2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**합성곱 신경망 구현**\n","\n","28x28 크기의 흑백 이미지와 3x3 크기의 커널 10개로 합성곱을 수행할 것이다.   \n","그 다음 2x2 크기의 최대풀링을 수행하여 14x14x10으로 특성 맵의 크기를 줄인다.   \n","이후 이 특성 맵을 일렬로 펼쳐 100개의 뉴런의 완전연결층과 연결시킨다.   \n","그 다음 10개의 클래스를 구분하기 위해 소프트맥스 함수를 연결한다.   \n"],"metadata":{"id":"oDkB-cMJo2VF"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"PMKOrJNaonx1","executionInfo":{"status":"ok","timestamp":1652348836992,"user_tz":-540,"elapsed":3792,"user":{"displayName":"정환희","userId":"10718546972386594771"}}},"outputs":[],"source":["# import dependencies\n","\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","\n","# MultiClassNetwork\n","# MultiClassNetwork class\n","\n","class MultiClassNetwork:\n","\n","  def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n","    self.units = units  # 은닉층 개수\n","    self.w1 = None      # 은닉층 가중치\n","    self.b1 = None      # 은닉층 절편\n","    self.w2 = None      # 출력층 가중치\n","    self.b2 = None      # 출력층 절편\n","    self.a1 = None      # 은닉층 활성화 출력\n","    self.losses = []    # 훈련세트 손실 리스트\n","    self.val_losses = []  # 검증세트 손실 리스트\n","    self.lr = learning_rate  # 학습률\n","    self.l1 = l1  # L1 규제값\n","    self.l2 = l2  # L2 규제값\n","    self.batch_size = batch_size  # 배치 사이즈\n","\n","\n","  def init_weights(self, n_features, n_classes):  # 가중치 초기화\n","    self.w1 = np.random.normal(0, 1, (n_features, self.units))  # (입력 특성 개수, 은닉층 유닛 개수)\n","    self.b1 = np.zeros(self.units)  # 은닉층 유닛 개수\n","    self.w2 = np.random.normal(0, 1, (self.units, n_classes)) # (은닉층 유닛 개수, 분류 클래스 개수(출력층 유닛))\n","    self.b2 = np.zeros(n_classes) # 분류 클래스 개수(출력층 유닛)\n","\n","\n","  def forpass(self, x):   # 정방향 계산\n","    z1 = np.dot(x, self.w1) + self.b1   # z1 = XW + B (은닉층 계산)\n","    self.a1 = self.sigmoid(z1)          # 시그모이드 함수 통과\n","    z2 = np.dot(self.a1, self.w2) + self.b2   # z2 = AW + B (출력층 계산)\n","    return z2\n","\n","  \n","  def backprop(self, x, err): # 역방향 계산\n","    m = len(x)\n","    w2_grad = np.dot(self.a1.T, err) / m  # 출력층 가중치 그레이디언트  A1.T(-(Y - A2)) --> (-(Y - A2)) = err\n","    b2_grad = np.sum(err) / m     \n","    err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)  # 은닉층 손실함수 미분 오차량 -(Y - A2)W2.T * A1 * (1 - A1)\n","    w1_grad = np.dot(x.T, err_to_hidden) / m      # 은닉층 가중치 그레이디언트 X.T(err) / m\n","    b1_grad = np.sum(err_to_hidden, axis=0) / m\n","    return w1_grad, b1_grad, w2_grad, b2_grad\n","\n","\n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):  # 피팅 함수\n","    np.random.seed(42)  \n","    self.init_weights(x.shape[1], y.shape[1])   # 가중치 초기화(특성, 분류 클래스 개수)\n","    for i in range(epochs):\n","      loss = 0\n","      print('.', end=' ')\n","      for x_batch, y_batch in self.gen_batch(x, y):   # 배치만큼 반복\n","        a = self.training(x_batch, y_batch)\n","        a = np.clip(a, 1e-10, 1-1e-10)\n","        loss += np.sum(-y_batch * np.log(a))\n","      self.losses.append((loss + self.reg_loss()) / len(x))\n","      self.update_val_loss(x_val, y_val)\n","\n","\n","  def gen_batch(self, x, y):\n","    length = len(x)\n","    bins = length // self.batch_size\n","    if length % self.batch_size:\n","      bins += 1\n","    indexes = np.random.permutation(np.arange(len(x)))\n","    x = x[indexes]\n","    y = y[indexes]\n","    for i in range(bins):\n","      start = self.batch_size * i\n","      end = self.batch_size * (i + 1)\n","      yield x[start:end], y[start:end]\n","\n","\n","  def sigmoid(self, z):\n","    z = np.clip(z, -100, None)\n","    a = 1 / (1 + np.exp(-z))\n","    return a\n","\n","\n","  def training(self, x, y):   # 훈련 함수\n","    m = len(x)  \n","    z = self.forpass(x)   # 정방향 계산 (출력층 z)\n","    a = self.softmax(z)   # 소프트맥스  (출력층 결과값 정규화)\n","    err = -(y - a)        # 크로스엔트로피 손실함수 미분을 위한 오차량 계산\n","    w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)  # 역방향 계산\n","    w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m   # 은닉층 규제 적용\n","    w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m   # 출력층 규제 적용\n","    self.w1 -= self.lr * w1_grad\n","    self.b1 -= self.lr * b1_grad\n","    self.w2 -= self.lr * w2_grad\n","    self.b2 -= self.lr * b2_grad\n","    return a\n","\n","\n","  def softmax(self, z):\n","    z = np.clip(z, -100, None)\n","    exp_z = np.exp(z)\n","    return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n","\n","\n","  def predict(self, x):\n","    z = self.forpass(x)\n","    return np.argmax(z, axis=1)\n","\n","\n","  def score(self, x, y):\n","    return np.mean(self.predict(x) == np.argmax(y, axis=1))\n","\n","\n","  def reg_loss(self):\n","    return self.l1 * (np.sum(abs(self.w1))) + np.sum(np.abs(self.w2)) + self.l2 / 2 * np.sum(self.w1**2) + np.sum(self.w2**2) # 규제 손실\n","\n","  \n","  def update_val_loss(self, x_val, y_val):\n","    z = self.forpass(x_val)\n","    a = self.softmax(z)\n","    a = np.clip(a, 1e-10, 1-1e-10)\n","    val_loss = np.sum(-y_val * np.log(a))\n","    self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))"]},{"cell_type":"markdown","source":["#**구현**\n","\n","MultiClassNetwork 클래스의 forpass() 메서드에 있던 z1, a1, z2를 계산하는 식은 그대로 두고,   \n","그 앞에 함성곱과 풀링층을 추가해보자."],"metadata":{"id":"6IhLXbyEp_gV"}},{"cell_type":"markdown","source":["###**변경점**\n","\n",">1. 합성곱층을 통과한 특성 맵 c_out 설정(line:26)\n","2. 렐루함수를 적용한 특성 맵 r_out 설정(line:27)\n","3. 풀링층을 통과한 특성 맵 p_out 설정(line:28)\n","4. 특성 맵을 완전연결층과 연결하기 위해 일렬로 펼친(배치 차원 제외) f_put 설정(line:29)\n","5. np.dot() 대신 tf.matmul()로 대체 <- conv2d, max_pool2d()가 Tensor 객체를 반환하기 때문\n","6. sigmoid 대신 relu함수 사용\n","\n"],"metadata":{"id":"uDibtIMmsvC5"}},{"cell_type":"code","source":["class ConvolutionNetwork:\n","\n","  def __init__(self, n_kernels=10, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n","    self.n_kernels = n_kernels  # 합성곱 커널 개수\n","    self.kernel_size = 3  # 커널 크기\n","    self.optimizer = None # 옵티마이저\n","    self.conv_w = None\n","    self.conv_b = None\n","    self.units = units  # 은닉층 개수\n","    self.w1 = None      # 은닉층 가중치\n","    self.b1 = None      # 은닉층 절편\n","    self.w2 = None      # 출력층 가중치\n","    self.b2 = None      # 출력층 절편\n","    self.a1 = None      # 은닉층 활성화 출력\n","    self.losses = []    # 훈련세트 손실 리스트\n","    self.val_losses = []  # 검증세트 손실 리스트\n","    self.lr = learning_rate  # 학습률\n","    self.batch_size = batch_size  # 배치 사이즈\n","\n","\n","  def init_weights(self, input_shape, n_classes):  # 가중치 초기화\n","    g = tf.initializers.glorot_uniform()\n","    self.conv_w = tf.Variable(g((3, 3, 1, self.n_kernels)))   # 3x3x1xn_kernels 4차원 배열\n","    self.conv_b = tf.Variable(np.zeros(self.n_kernels), dtype=float)\n","    n_features = 14 * 14 * self.n_kernels\n","    self.w1 = tf.Variable(g((n_features, self.units)))  # (입력 특성 개수, 은닉층 유닛 개수)\n","    self.b1 = tf.Variable(np.zeros(self.units), dtype=float)  # 은닉층 유닛 개수\n","    self.w2 = tf.Variable(g((self.units, n_classes))) # (은닉층 유닛 개수, 분류 클래스 개수(출력층 유닛))\n","    self.b2 = tf.Variable(np.zeros(n_classes), dtype=float) # 분류 클래스 개수(출력층 유닛)\n","\n","\n","  def forpass(self, x):   # 정방향 계산\n","    c_out = tf.nn.conv2d(x, self.conv_w, strides=1, padding='SAME') + self.conv_b   # 3x3 합성곱 연산\n","    r_out = tf.nn.relu(c_out)   # 렐루 함수 적용\n","    p_out = tf.nn.max_pool2d(r_out, ksize=2, strides=1, padding='VALID')    # 2x2 풀링\n","    f_out = tf.reshape(p_out, [x.shape[0], -1])   # 첫 번째 배치 차원을 제외하고 출력을 일렬로 펼침\n","    z1 = tf.matmul(f_out, self.w1) + self.b1   # z1 = XW + B (은닉층 계산)  np.dot 대신 tf.matmul 사용\n","    self.a1 = tf.nn.relu(z1)          # relu 함수 통과\n","    z2 = tf.matmul(self.a1, self.w2) + self.b2   # z2 = AW + B (출력층 계산)  np.dot -> tf.matmul\n","    return z2\n","\n","\n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):  # 피팅 함수\n","    np.random.seed(42)  \n","    self.init_weights(x.shape[1], y.shape[1])   # 가중치 초기화(특성, 분류 클래스 개수)\n","    self.optimizer = tf.optimizers.SGD(learning_rate=self.lr)   # SGD(확률적 경사 하강법) 옵티마이저\n","    for i in range(epochs):\n","      print('에포크', i , end=' ')\n","      batch_losses = []\n","      for x_batch, y_batch in self.gen_batch(x, y):   # 배치만큼 반복\n","        print('.', end='')\n","        self.training(x_batch, y_batch)\n","        batch_losses.append(self.get_loss(x_batch, y_batch))\n","      print()\n","      self.losses.append(np.mean(batch_losses))\n","      self.val_losses.append(self.get_loss(x_val, y_val))\n","\n","\n","  def gen_batch(self, x, y):\n","    length = len(x)\n","    bins = length // self.batch_size\n","    if length % self.batch_size:\n","      bins += 1\n","    indexes = np.random.permutation(np.arange(len(x)))\n","    x = x[indexes]\n","    y = y[indexes]\n","    for i in range(bins):\n","      start = self.batch_size * i\n","      end = self.batch_size * (i + 1)\n","      yield x[start:end], y[start:end]\n","\n","\n","  def training(self, x, y):   # 훈련 함수\n","    m = len(x)  \n","    with tf.GradientTape() as tape:\n","      z = self.forpass(x)   # 정방향 계산 (출력층 z)\n","      loss = tf.nn.softmax_cross_entropy_with_logits(y, z)    # 손실 계산(각 샘플에 대한 손실)\n","      loss = tf.reduce_mean(loss)   # 손실의 평균 구하기\n","    weights_list = [self.conv_w, self.conv_b, self.w1, self.b1, self.w2, self.b2]\n","    grads = tape.gradient(loss, weights_list)   # 가중치 그레이디언트 계산\n","    self.optimizer.apply_gradients(zip(grads, weights_list))  # 가중치 업데이트\n","\n","\n","  def predict(self, x):\n","    z = self.forpass(x)\n","    return np.argmax(z, axis=1)\n","\n","\n","  def score(self, x, y):\n","    return np.mean(self.predict(x) == np.argmax(y, axis=1))\n","\n","\n","  def get_loss(self, x, y):\n","    z = self.forpass(x)\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, z))\n","    return loss.numpy()\n"],"metadata":{"id":"efQf0nhSqRnD","executionInfo":{"status":"ok","timestamp":1652351828662,"user_tz":-540,"elapsed":318,"user":{"displayName":"정환희","userId":"10718546972386594771"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#**합성곱 신경망 훈련하기**\n","\n","패션 MNIST 데이터셋으로 훈련시켜보자."],"metadata":{"id":"YSAKKddV2NMJ"}},{"cell_type":"code","source":["(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)\n","\n","y_train_encoded = tf.keras.utils.to_categorical(y_train)\n","y_val_encoded = tf.keras.utils.to_categorical(y_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kniqgRHD2VqY","executionInfo":{"status":"ok","timestamp":1652352087645,"user_tz":-540,"elapsed":1555,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"68b728a3-fcb0-4673-bef2-ff1d1d1c668a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","40960/29515 [=========================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","26435584/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","16384/5148 [===============================================================================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","4431872/4422102 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["x_train = x_train.reshape(-1, 28, 28, 1)\n","x_val = x_val.reshape(-1, 28, 28, 1)\n","\n","x_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFmCvWDU2-zb","executionInfo":{"status":"ok","timestamp":1652352174164,"user_tz":-540,"elapsed":11,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"9531da45-dc19-4412-b165-6edc72be100b"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(48000, 28, 28, 1)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["x_train = x_train / 255\n","x_val = x_val / 255"],"metadata":{"id":"bcU2X7NR3UST","executionInfo":{"status":"ok","timestamp":1652352376712,"user_tz":-540,"elapsed":312,"user":{"displayName":"정환희","userId":"10718546972386594771"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"O1Q6IC1f4FvT"},"execution_count":null,"outputs":[]}]}