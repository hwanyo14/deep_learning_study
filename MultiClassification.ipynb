{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Doit_MultiClassification.ipynb","provenance":[],"collapsed_sections":["vmBTmz2FHlZm"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**다중 분류**\n","\n","다중 분류는 각 클래스에 대한 확률값을 출력한다.   \n","> 예를 들어 첫 번째 클래스에 대한 확률은 $ŷ_1$,   \n"," 두 번째 클래스에 대한 확률은 $ŷ_2$,   \n"," 세 번째 클래스에 대한 확률은 $ŷ_3$\n","\n","다중 분류 신경망은 출력층에 분류할 클래스의 개수만큼 뉴런을 배치한다.\n","(ex> 분류 항목이 3 개면 출력층 뉴런이 3 개)"],"metadata":{"id":"OqJPVCic_ptA"}},{"cell_type":"markdown","source":["#**Softmax Function: 소프트맥스 함수**\n","\n","출력층의 출력 강도를 정규화하는 함수   \n","출력 강도를 정규화한다는 것은 '모든 출력값의 합을 1로 만든다'는 의미이다.   \n","###$$\n","  Softmax\\_func = \\frac{𝑒^{z_i}}{𝑒^{z_1} + 𝑒^{z_2} + 𝑒^{z_3}}\n","$$\n","출력이 늘어나는 만큼 지수 함수가 늘어난다.   \n","ex> 두 번째 클래스에 대한 소프트맥스 함수 = $\\frac{𝑒^{z_2}}{𝑒^{z_1} + 𝑒^{z_2} + 𝑒^{z_3}}$"],"metadata":{"id":"8tQnc6xACky6"}},{"cell_type":"markdown","source":["#**Cross Entropy: 크로스 엔트로피 손실함수**\n","\n","**'크로스 엔트로피 손실함수'**는 다중 분류에서 사용하는 손실함수   \n","####**'로지스틱 손실함수의 일반화 버전'**\n",">로지스틱 손실함수가 크로스 엔트로피 손실함수의 이진분류 버전이기 때문이다.\n","\n","###$$\n","  L = -\\displaystyle\\sum_{c=1}^{c}y_clog(a_c) = -(y_1log(a_1) + y_2log(a_2) + … + y_clog(a_c))\n","   = -1 × log(a_{y = 1})\n","$$\n","\n","c 값은 전체 클래스 개수를 의미한다.    \n","$a_{y=1}$는 정답 클래스에 해당하는 활성화 출력이다."],"metadata":{"id":"MlhZEIrBFRKh"}},{"cell_type":"markdown","source":["#**크로스 엔트로피 손실함수 미분**\n","\n","경사하강법을 적용하기 위해 미분을 해보자.\n","\n","###**$z_1$에 대하여 미분**\n","###$$\n","  \\frac{∂L}{∂z_1} = \\frac{∂L}{∂a_1}\\frac{∂a_1}{∂z_1} + \\frac{∂L}{∂a_2}\\frac{∂a_2}{∂z_1} + \\frac{∂L}{∂a_3}\\frac{∂a_3}{∂z_1}\n","$$\n","<br>\n","\n","하나씩 순서대로 미분을 진행해보자.\n","\n","###$$\n","  \\frac{∂L}{∂a_1} = -\\frac{∂}{∂a_1}(y_1log\\,a_1 + y_2log\\,a_2 + y_3log\\,a_3)\n","  = -\\frac{y_1}{a_1}\n"," $$\n","<br>\n"," \n","$a_1$에 관한식 $y_1log\\,a_1$만 남아 미분된다.   \n","마찬가지 방법으로 $a_2,\\, a_3$도 미분해보자.\n","\n","###$$\n","  \\frac{∂L}{∂a_2} = -\\frac{y_2}{a_2} \\quad\\quad \\frac{∂L}{∂a_3} = -\\frac{y_3}{a_3}\n"," $$\n","<br>\n","\n","이 식을 $\\frac{∂L}{∂z_1}$에 대입하면,\n","\n","###$$\n","  \\frac{∂L}{∂z_1} = (-\\frac{y_1}{a_1})\\frac{∂a_1}{∂z_1} + (-\\frac{y_2}{a_2})\\frac{∂a_2}{∂z_1} + (-\\frac{y_3}{a_3})\\frac{∂a_3}{∂z_1}\n","$$\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","\n","##머 여차저차 하여 식을 쭈루루룩 두비두밥 두밥바 두구두구 짠짠 $z_1$송송 미분탁 하면\n","\n","###$$\n","  \\frac{∂L}{∂z} = -(𝒚 - 𝐚)\n","$$\n","\n","로지스틱 손실함수와 정확히 일치한다.\n","\n","따라서 크로스 엔트로피 손실함수를 역전파에 사용하기 위해 따로 코드를 수정할 필요는 없다.   \n","\n","이제 직접 MultiClassNetwork 클래스를 구현해보자."],"metadata":{"id":"vmBTmz2FHlZm"}},{"cell_type":"code","source":["# import dependencies\n","\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# divid samples into train, val, test sets\n","\n","cancer = load_breast_cancer()\n","c_data = cancer.data\n","c_target = cancer.target\n","train_data_all, test_data, train_target_all, test_target = train_test_split(c_data, c_target, stratify=c_target, test_size=0.2, random_state=42)\n","train_data, val_data, train_target, val_target = train_test_split(train_data_all, train_target_all, stratify=train_target_all, test_size=0.2, random_state=42)\n","print(train_data.shape, val_data.shape)\n","\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","scaler.fit(train_data)  # train_data를 넣어 fit()하여 변환 규칙을 익히고\n","train_data_scaled = scaler.transform(train_data)  # 표준화 전처리\n","val_data_scaled = scaler.transform(val_data)\n","\n","\n","class SingleLayer:\n","\n","  def __init__(self, learning_rate=0.1, l1=0, l2=0):\n","    self.w = None   # 입력데이터의 특성이 많아 가중치와 절편을 미리 초기화하지 않는다.\n","    self.b = None   # 나중에 입력데이터를 보고 특성 개수에 맞게 결정\n","    self.losses = []\n","    self.w_history = []   # 가중치를 저장할 리스트\n","    self.lr = learning_rate   # 학습률 \n","    self.val_losses = []    # 검증세트 손실을 기록할 리스트\n","    self.l1 = l1\n","    self.l2 = l2\n","\n","  def forpass(self, x):\n","    z = np.dot(x, self.w) + self.b   # x=(364, 30) dot w=(30, 1) = z=(364, 1)\n","    return z\n","\n","\n","  def backprop(self, x, err):   # 오차역전파 메서드\n","    m = len(x)\n","    w_grad = np.dot(x.T, err) / m # x.T=(364, 30) dot err=(30, 1) = w_grad=(364, 1)\n","    b_grad = np.sum(err) / m  # err/m = (364, 1)\n","    return w_grad, b_grad\n","\n","\n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):  # 검증세트를 전달받을 x_val, y_val 추가\n","    y = y.reshape(-1, 1)  # 타깃을 열벡터로 바꾼다.\n","    y_val = y_val.reshape(-1, 1)  # 검증타깃을 열벡터로 바꾼다.\n","    m = len(x)\n","    self.w = np.ones((x.shape[1], 1))  # 가중치와 절편 초기화\n","    self.b = 0\n","    self.w_history.append(self.w.copy())  # 가중치 기록 -> 넘파이 배열(w)을 추가하면 실제값이 추가되는 것이 아닌 배열을 참조하기 때문에 w값이 바뀌면 그 값을 복사하여 추가해주어야 한다.\n","    np.random.seed(42)\n","    for i in range(epochs):\n","      z = self.forpass(x)   # 정방향 계산\n","      a = self.activation(z)  # 정방향 계산의 결과값인 z를 활성화 함수에 통과\n","      err =  -(y - a)       # 활성화 함수를 거친 a값으로 오차량 계산\n","      w_grad, b_grad = self.backprop(x, err)  # 오차역전파\n","      w_grad += (self.l1 * np.sign(self.w) + self.l2 * self.w) / m # 그레이디언트에서 패널티 항의 미분값을 더한다.\n","      self.w -= self.lr * w_grad   # 그레이디언트 업데이트  (학습률 적용)\n","      self.b -= self.lr * b_grad\n","      self.w_history.append(self.w.copy())  # 가중치 기록\n","      a = np.clip(a, 1e-10, 1-1e-10)\n","      loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))  # 로지스틱 손실함수 -(ylog(a) - (1 - y)log(1 - a))\n","      self.losses.append((loss + self.reg_loss()) / m)\n","      self.update_val_loss(x_val, y_val)  # 검증세트 손실을 업데이트하는 메서드 호출\n","\n","\n","  def reg_loss(self):\n","    return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)\n","\n","  \n","  def update_val_loss(self, x_val, y_val):\n","    z = self.forpass(x_val)\n","    a = self.activation(z)\n","    a = np.clip(a, 1e-10, 1-1e-10)\n","    val_loss = np.sum(-(y_val * np.log(a) + (1 - y_val) * np.log(1 - a)))\n","    self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))\n","\n","\n","  def activation(self, z):  # 활성화 함수\n","    z = np.clip(z, -100, None)    # 안전한 계산을 위해 클리핑\n","    a = 1 / (1 + np.exp(-z))\n","    return a\n","\n","\n","  def predict(self, x): # 예측 함수\n","    z = self.forpass(x)\n","    return z > 0  # 계단함수\n","\n","\n","  def score(self, x, y):    # 평가함수\n","    return np.mean(self.predict(x) == y.reshape(-1, 1))\n","\n","\n","class DualLayer(SingleLayer):\n","\n","  def __init__(self, units=10, learning_rate=0.1, l1=0, l2=0):\n","    self.units = units  # 은닉층 유닛 개수\n","    self.w1 = None      # 은닉층의 가중치\n","    self.b1 = None      # 은닉층의 절편\n","    self.w2 = None      # 출력층의 가중치\n","    self.b2 = None      # 출력층의 절편\n","    self.a1 = None      # 은닉층의 활성화 출력\n","    self.losses = []    # 훈련 손실\n","    self.val_losses = []  # 검증 손실\n","    self.lr = learning_rate # 학습률\n","    self.l1 = l1    # L1 손실 파라미터\n","    self.l2 = l2    # L2 손실 파라미터\n","\n","  \n","  def forpass(self, x):\n","    z1 = np.dot(x, self.w1) + self.b1\n","    self.a1 = self.activation(z1)\n","    z2 = np.dot(self.a1, self.w2) + self.b2\n","    return z2\n","\n","  \n","  def backprop(self, x, err):\n","    m = len(x)\n","    w2_grad = np.dot(self.a1.T, err) / m  # 출력층 그레이디언트\n","    b2_grad = np.sum(err) / m\n","    err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)  # 시그모이드 함수까지 그레이디언트 계산\n","    w1_grad = np.dot(x.T, err_to_hidden) / m\n","    b1_grad = np.sum(err_to_hidden, axis=0) / m\n","    return w1_grad, b1_grad, w2_grad, b2_grad\n","\n","\n","  def init_weights(self, n_features):\n","    self.w1 = np.ones((n_features, self.units))   # 가중치 행렬 초기화 (특성개수, 뉴런 개수(은닉층 크기))\n","    self.b1 = np.zeros(self.units)  # 뉴런 개수(은닉층 크기)\n","    self.w2 = np.ones((self.units, 1))  # (뉴런 개수(은닉층 크기), 1)\n","    self.b2 = 0\n","\n","  \n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n","    y = y.reshape(-1, 1)\n","    y_val = y_val.reshape(-1, 1)\n","    m = len(x)\n","    self.init_weights(x.shape[1])\n","    for i in range(epochs):\n","      a = self.training(x, y, m)\n","      a = np.clip(a, 1e-10, 1-1e-10)\n","      loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))\n","      self.losses.append((loss + self.reg_loss()) / m)\n","      self.update_val_loss(x_val, y_val)\n","\n","  \n","  def training(self, x, y, m):\n","    z = self.forpass(x)\n","    a = self.activation(z)\n","    err = -(y - a)\n","    w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n","    w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n","    w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n","    self.w1 -= self.lr * w1_grad\n","    self.b1 -= self.lr * b1_grad\n","    self.w2 -= self.lr * w2_grad\n","    self.b2 -= self.lr * b2_grad\n","    return a\n","\n","\n","  def reg_loss(self):\n","    return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n","            self.l2 / 2 *(np.sum(self.w1**2) + np.sum(self.w2**2))\n","\n","\n","class RandomInitNetwork(DualLayer):\n","\n","  def init_weights(self, n_features):\n","    np.random.seed(42)\n","    self.w1 = np.random.normal(0, 1, (n_features, self.units))  # np.random.normal(평균, 표준편차, 배열크기)\n","    self.b1 = np.zeros(self.units)\n","    self.w2 = np.random.normal(0, 1, (self.units, 1))\n","    self.b2 = 0\n","\n","\n","class MiniBatchNetwork(RandomInitNetwork):\n","\n","  def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n","    super().__init__(units, learning_rate, l1, l2)\n","    self.batch_size = batch_size\n","\n","  \n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n","    y_val = y_val.reshape(-1, 1)\n","    self.init_weights(x.shape[1])\n","    np.random.seed(42)\n","    for i in range(epochs):\n","      loss = 0\n","      for x_batch, y_batch in self.gen_batch(x, y):\n","        y_batch = y_batch.reshape(-1, 1)\n","        m = len(x_batch)\n","        a = self.training(x_batch, y_batch, m)\n","        a = np.clip(a, 1e-10, 1-1e-10)\n","        loss += np.sum(-(y_batch * np.log(a) + (1 - y_batch) * np.log(1 - a)))\n","      self.losses.append((loss + self.reg_loss()) / len(x))\n","      self.update_val_loss(x_val, y_val)\n","\n","\n","  def gen_batch(self, x, y):\n","    length = len(x)\n","    bins = length // self.batch_size\n","    if length % self.batch_size:\n","      bins += 1\n","    indexes = np.random.permutation(np.arange(len(x)))\n","    x = x[indexes]\n","    y = y[indexes]\n","    for i in range(bins):\n","      start = self.batch_size * i\n","      end = self.batch_size * (i + 1)\n","      yield x[start:end], y[start:end]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuHChMCcON3B","executionInfo":{"status":"ok","timestamp":1651795090729,"user_tz":-540,"elapsed":1159,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"5a98db0c-b514-49bb-be94-4e84e7b2c710"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(364, 30) (91, 30)\n"]}]},{"cell_type":"markdown","source":["#**MultiClassNetwork 클래스 만들기**\n","\n","1. 소프트맥스 함수 추가하기\n",">activation() 메서드 이름을 sigmoid()로 바꾸고, softmax() 메서드를 추가한다.\n","\n","<br><br>\n","\n","##**다중 분류 신경망 flow**\n","1. 은닉층 유닛 개수, 배치 사이즈, 학습률, 규제값을 입력받아 객체 생성\n","2. 데이터 샘플과 타깃, 에포크, 검증 샘플과 타깃을 입력받아 fit\n","3. 데이터 샘플(입력 샘플 개수)와 타깃(분류될 클래스 개수)를 바탕으로 가중치 초기화(init_weights)\n","4. 에포크만큼 배치를 생성(gen_batch)하여 training\n","  1. 데이터 샘플 배열 $𝑋$로 정방향 계산\n","    1. 은닉층 계산 $𝑍_1 = 𝑋𝑊_1 + 𝑏_1$ \n","    ```\n","    z1 = np.dot(x, self.w1) + self.b1\n","    ```\n","    2. 은닉층 정방향 출력값 z1을 시그모이드 함수에 적용\n","    ```\n","    a1 =  self.sigmoid(z1)\n","    ```\n","    3. 출력층 계산 $𝑍_2 = 𝐴_1𝑊_2 + 𝑏_2$\n","    ```\n","    z2 = np.dot(self.a1, self.w2) + self.b2\n","    ```\n","  2. 출력층 결과값 z2를 소프트맥스 함수에 적용\n","    $softmax = \\frac{𝑒^{zi}}{𝑒^{z1} + 𝑒^{z2} + 𝑒^{z3}}$\n","    ```\n","    a = exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n","    ```\n","  3. 오차량 계산 \n","  ```\n","    err = -(y - a)\n","  ```\n","  4. 데이터 샘플과 오차량을 이용해 역방향 계산\n","    1. 출력층 가중치 그레이디언트(w2_grad) 계산$\\quad𝐴_1^𝑇(-(𝑌 - 𝐴_2))$   \n","    ($𝐴_1^𝑇(-(𝑌 - 𝐴_2))$는 모든 샘플에 대한 가중치 그레이디언트이므로, 전체 샘플의 수로 나누어줘야 함)\n","    ```\n","      w2_grad = np.dot(self.a1.T, err) / len(x)\n","    ```\n","    2. 출력층 절편 그레이디언트(b2_grad) 계산 $\\quad 𝟏^𝑇(-(𝑌 - 𝐴_2))$\n","    ```\n","      b2_grad = (np.sum(err)) / len(x)\n","    ```\n","    3. 은닉층 그레이디언트 계산을 위해 은닉층 오차량(err_to_hidden) 계산   \n","      $(-(𝑌 - 𝐴_2)𝑊_2^𝑇⊙𝐴_1⊙(1 - 𝐴_1)) \\quad ※-(𝑌 - 𝐴_2) =$ err\n","      ```\n","        err_to_hidden = np.dot(err, self.w2) * self.a1 * (1 - self.a1)\n","      ```\n","    4. 은닉층 가중치 그레이디언트 계산   \n","      "],"metadata":{"id":"sn1LBcG2N4fn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZneHjxGK_luB"},"outputs":[],"source":["# MultiClassNetwork class\n","\n","class MultiClassNetwork:\n","\n","  def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n","    self.units = units  # 은닉층 개수\n","    self.w1 = None      # 은닉층 가중치\n","    self.b1 = None      # 은닉층 절편\n","    self.w2 = None      # 출력층 가중치\n","    self.b2 = None      # 출력층 절편\n","    self.a1 = None      # 은닉층 활성화 출력\n","    self.losses = []    # 훈련세트 손실 리스트\n","    self.val_losses = []  # 검증세트 손실 리스트\n","    self.lr = learning_rate  # 학습률\n","    self.l1 = l1  # L1 규제값\n","    self.l2 = l2  # L2 규제값\n","    self.batch_size = batch_size  # 배치 사이즈\n","\n","\n","  def init_weights(self, n_features, n_classes):  # 가중치 초기화\n","    self.w1 = np.random.normal(0, 1, (n_features, self.units))  # (입력 특성 개수, 은닉층 유닛 개수)\n","    self.b1 = np.zeros(self.units)  # 은닉층 유닛 개수\n","    self.w2 = np.random.normal(0, 1, (self.units, n_classes)) # (은닉층 유닛 개수, 분류 클래스 개수(출력층 유닛))\n","    self.b2 = np.zeros(n_classes) # 분류 클래스 개수(출력층 유닛)\n","\n","\n","  def forpass(self, x):   # 정방향 계산\n","    z1 = np.dot(x, self.w1) + self.b1   # z1 = XW + B (은닉층 계산)\n","    self.a1 = self.sigmoid(z1)          # 시그모이드 함수 통과\n","    z2 = np.dot(self.a1, self.w2) + self.b2   # z2 = AW + B (출력층 계산)\n","    return z2\n","\n","  \n","  def backprop(self, x, err): # 역방향 계산\n","    m = len(x)\n","    w2_grad = np.dot(self.a1.T, err) / m  # 출력층 가중치 그레이디언트  A1.T(-(Y - A2)) --> (-(Y - A2)) = err\n","    b2_grad = np.sum(err) / m     \n","    err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)  # 은닉층 손실함수 미분 오차량 -(Y - A2)W2.T * A1 * (1 - A1)\n","    w1_grad = np.dot(x.T, err_to_hidden) / m      # 은닉층 가중치 그레이디언트 X.T(err) / m\n","    b1_grad = np.sum(err_to_hidden, axis=0) / m\n","    return w1_grad, b1_grad, w2_grad, b2_grad\n","\n","\n","  def fit(self, x, y, epochs=100, x_val=None, y_val=None):  # 피팅 함수\n","    np.random.seed(42)  \n","    self.init_weights(x.shape[1], y.shape[1])   # 가중치 초기화(특성, 분류 클래스 개수)\n","    for i in range(epochs):\n","      loss = 0\n","      print('.', end=' ')\n","      for x_batch, y_batch in self.gen_batch(x, y):   # 배치만큼 반복\n","        a = self.training(x_batch, y_batch)\n","        a = np.clip(a, 1e-10, 1-1e-10)\n","        loss += np.sum(-y_batch * np.log(a))\n","      self.losses.append((loss + self.reg_loss()) / len(x))\n","      self.update_val_loss(x_val, y_val)\n","\n","\n","  def gen_batch(self, x, y):\n","    length = len(x)\n","    bins = length // self.batch_size\n","    if length % self.batch_size:\n","      bins += 1\n","    indexes = np.random.permutation(np.arange(len(x)))\n","    x = x[indexes]\n","    y = y[indexes]\n","    for i in range(bins):\n","      start = self.batch_size * i\n","      end = self.batch_size * (i + 1)\n","      yield x[start:end], y[start:end]\n","\n","\n","  def sigmoid(self, z):\n","    z = np.clip(z, -100, None)\n","    a = 1 / (1 + np.exp(-z))\n","    return a\n","\n","\n","  def training(self, x, y):   # 훈련 함수\n","    m = len(x)  \n","    z = self.forpass(x)   # 정방향 계산 (출력층 z)\n","    a = self.softmax(z)   # 소프트맥스  (출력층 결과값 정규화)\n","    err = -(y - a)        # 크로스엔트로피 손실함수 미분을 위한 오차량 계산\n","    w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)  # 역방향 계산\n","    w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m   # 은닉층 규제 적용\n","    w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m   # 출력층 규제 적용\n","    self.w1 -= self.lr * w1_grad\n","    self.b1 -= self.lr * b1_grad\n","    self.w2 -= self.lr * w2_grad\n","    self.b2 -= self.lr * b2_grad\n","    return a\n","\n","\n","  def softmax(self, z):\n","    z = np.clip(z, -100, None)\n","    exp_z = np.exp(z)\n","    return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n","\n","\n","  def predict(self, x):\n","    z = self.forpass(x)\n","    return np.argmax(z, axis=1)\n","\n","\n","  def score(self, x, y):\n","    return np.mean(self.predict(x) == np.argmax(y, axis=1))\n","\n","\n","  def reg_loss(self):\n","    return self.l1 * (np.sum(abs(self.w1))) + np.sum(np.abs(self.w2)) + self.l2 / 2 * np.sum(self.w1**2) + np.sum(self.w2**2) # 규제 손실\n","\n","  \n","  def update_val_loss(self, x_val, y_val):\n","    z = self.forpass(x_val)\n","    a = self.softmax(z)\n","    a = np.clip(a, 1e-10, 1-1e-10)\n","    val_loss = np.sum(-y_val * np.log(a))\n","    self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))"]},{"cell_type":"markdown","source":["#**의류 이미지 분류**\n","\n","패션 MNIST 데이터셋을 이용해 의류를 분류해보자."],"metadata":{"id":"Hch97-KwVr1L"}},{"cell_type":"code","source":["!pip install tensorflow_gpu==2.0.0  # 탠서플로 최신버전 설지"],"metadata":{"id":"1XqUwFzQVptY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"eNPXSPB2V83V","executionInfo":{"status":"ok","timestamp":1651795058921,"user_tz":-540,"elapsed":3371,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"3737099d-7fc7-4a40-b062-eb3dbb4be85a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["(train_data_all, train_target_all), (test_data, test_target) = tf.keras.datasets.fashion_mnist.load_data()\n","'''\n","  load_data()는 훈련세트와 테스트세트를 튜플로 묶어 반환한다.\n","  여기서 우리는 훈련세트만 사용한다.\n","  테스트세트를 사용해 모델을 반복적으로 평가하면 실전에 투입했을 때 성능을 낙관적으로 예측할 수 있기 때문이다.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"r0iNjtdObSCD","executionInfo":{"status":"ok","timestamp":1651795146285,"user_tz":-540,"elapsed":823,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"bf199b25-cc70-49e9-b28b-affbfb02b3b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n  load_data()는 훈련세트와 테스트세트를 튜플로 묶어 반환한다.\\n  여기서 우리는 훈련세트만 사용한다.\\n  테스트세트를 사용해 모델을 반복적으로 평가하면 실전에 투입했을 때 성능을 낙관적으로 예측할 수 있기 때문이다.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print(train_data_all.shape, train_target_all.shape)\n","'''\n","  각 샘플은 높이와 너비가 있는 2차원 흑백 이미지이고,\n","  train_data_all은 2차원 이미지 데이터를 모아놓은 세트이기 때문에 3차원 배열이다.\n","  28 X 28 사이즈의 이미지 60000장이 쌓여 있는 형태이다.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"gIlygAoNbi6R","executionInfo":{"status":"ok","timestamp":1651795148781,"user_tz":-540,"elapsed":286,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"0b625fb9-2eda-444e-e751-a3a7a37bd0cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28) (60000,)\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n  각 샘플은 높이와 너비가 있는 2차원 흑백 이미지이고,\\n  train_data_all은 2차원 이미지 데이터를 모아놓은 세트이기 때문에 3차원 배열이다.\\n  28 X 28 사이즈의 이미지 60000장이 쌓여 있는 형태이다.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["plt.imshow(train_data_all[0], cmap='gray')\n","plt.show()\n","'''\n","  imshow() 함수는 넘파이 배열을 입력 받아 이미지를 그린다.\n","  matplotlib은 컬러맵(colormap)을 사용해 이미지를 그리고 cmap 파라미터로 설정할 수 있다.\n","  넘파이 배열 원소값이 0에 가까울수록 검은색이다.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"N-qT55uCb13D","executionInfo":{"status":"ok","timestamp":1651756288270,"user_tz":-540,"elapsed":408,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"58975684-8354-4ea4-f891-cc33fae99934"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"execute_result","data":{"text/plain":["'\\n  imshow() 함수는 넘파이 배열을 입력 받아 이미지를 그린다.\\n  matplotlib은 컬러맵(colormap)을 사용해 이미지를 그리고 cmap 파라미터로 설정할 수 있다.\\n  넘파이 배열 원소값이 0에 가까울수록 검은색이다.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["print(train_target_all[:10])\n","class_names = ['티셔츠/윗도리', '바지', '스웨터', '드레스', '코트', '샌들', '셔츠', '스니커즈', '가방', '앵클부츠']\n","print(class_names[train_target_all[3]])\n","\n","np.bincount(train_target_all) # 타깃 분포 -> 각 레이블당 6000개 씩 총 60000개"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ED_0t1pSdw80","executionInfo":{"status":"ok","timestamp":1651756290649,"user_tz":-540,"elapsed":10,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"0624ce73-7d10-48d2-ae5e-0944b3c06a2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[9 0 0 3 0 2 7 2 5 5]\n","드레스\n"]},{"output_type":"execute_result","data":{"text/plain":["array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 훈련세트 검증세트 분할\n","\n","train_data, val_data, train_target, val_target = train_test_split(train_data_all, train_target_all, stratify=train_target_all, test_size=0.2, random_state=42)\n","\n","print(np.bincount(train_target))\n","print(np.bincount(val_target))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e37qFoode7EH","executionInfo":{"status":"ok","timestamp":1651795155833,"user_tz":-540,"elapsed":294,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"c49bd70b-b5aa-4db7-c4aa-22fbac5823de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[4800 4800 4800 4800 4800 4800 4800 4800 4800 4800]\n","[1200 1200 1200 1200 1200 1200 1200 1200 1200 1200]\n"]}]},{"cell_type":"code","source":["# 입력 데이터 표준화\n","\n","train_data = train_data / 255\n","val_data = val_data / 255\n","\n","'''\n","  평균을 0, 분산을 1로 맞추는 것을 '표준화'라고 배웠다.\n","  여기서는 이미지의 RGB값이 0~255 이므로 각 픽셀을 255로 나누어주어\n","  0~1 사이 값으로 맞췄다.\n","  엄밀히 '표준화'라고 말할 수는 없지만, 실전에서 잘 작동하므로 이 방법을 많이 사용한다.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"MQIRU0GpgFUH","executionInfo":{"status":"ok","timestamp":1651795158014,"user_tz":-540,"elapsed":331,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"bd4af111-5910-4b90-a7be-038e43e32d77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n  평균을 0, 분산을 1로 맞추는 것을 '표준화'라고 배웠다.\\n  여기서는 이미지의 RGB값이 0~255 이므로 각 픽셀을 255로 나누어주어\\n  0~1 사이 값으로 맞췄다.\\n  엄밀히 '표준화'라고 말할 수는 없지만, 실전에서 잘 작동하므로 이 방법을 많이 사용한다.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# 훈련/검증세트 차원 변경\n","\n","train_data = train_data.reshape(-1, 784)\n","val_data = val_data.reshape(-1, 784)\n","\n","'''\n","  MultiClassNetwork 클래스는 1차원 배열 샘플을 입력받지만 준비한 데이터셋은 28x28 크기의 2차원 배열이다.\n","  따라서 reshape을 통해 샘플의 두 번째, 세 번째 차원을 합친 784(28 * 28)의 길이로 펼친다.\n","'''\n","\n","print(train_data.shape, val_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3h6jRBFBg4Cu","executionInfo":{"status":"ok","timestamp":1651795161211,"user_tz":-540,"elapsed":281,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"fc260bc0-07bf-4b53-a7d1-0491704d57b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(48000, 784) (12000, 784)\n"]}]},{"cell_type":"markdown","source":["패션 MNIST 데이터셋은 10개의 클래스로 구성되어 있으므로 출력층 뉴런이 10개여야 한다.   \n","하지만 train_target, val_target에 저장된 값은 0 ~ 9까지의 정수값 하나로 표현되어   \n","10개의 뉴런에 대응되지 않는다.   \n","우리는 이를 위해서 '원-핫 인코딩'을 사용해보자."],"metadata":{"id":"1Z80sZFhiIib"}},{"cell_type":"code","source":["# 원 핫 인코딩을 위한 함수\n","\n","tf.keras.utils.to_categorical([0, 1, 3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zAgqIMVThwxI","executionInfo":{"status":"ok","timestamp":1651756301714,"user_tz":-540,"elapsed":441,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"28ac129b-ded5-4dd1-9b40-04160ec643f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., 0.],\n","       [0., 1., 0., 0.],\n","       [0., 0., 0., 1.]], dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["train_target_encoded = tf.keras.utils.to_categorical(train_target)\n","val_target_encoded = tf.keras.utils.to_categorical(val_target)\n","\n","print(train_target_encoded.shape, val_target_encoded.shape)\n","print(train_target[0], train_target_encoded[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YDOIv-oizlc","executionInfo":{"status":"ok","timestamp":1651795165152,"user_tz":-540,"elapsed":278,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"94222a15-96af-4f15-c756-b75f238875ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(48000, 10) (12000, 10)\n","6 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"]}]},{"cell_type":"code","source":["# 모델 훈련하기\n","\n","fc = MultiClassNetwork(units=100, batch_size=256)\n","fc.fit(train_data, train_target_encoded, x_val=val_data, y_val=val_target_encoded, epochs=40)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPKe2aySjRXb","executionInfo":{"status":"ok","timestamp":1651756370831,"user_tz":-540,"elapsed":63919,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"36b55b26-ba26-43f6-e9f4-9b9dbdf7c524"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "]}]},{"cell_type":"code","source":["plt.plot(fc.losses)\n","plt.plot(fc.val_losses)\n","plt.ylabel('loss')\n","plt.xlabel('iteration')\n","plt.legend(['train_loss', 'val_loss'])\n","plt.show()"],"metadata":{"id":"PCmKMcO6krbV","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1651756448356,"user_tz":-540,"elapsed":18,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"dc954032-b866-4d73-9503-595b62c78e1a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["fc.score(val_data, val_target_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjWyUtliWzKI","executionInfo":{"status":"ok","timestamp":1651756498882,"user_tz":-540,"elapsed":291,"user":{"displayName":"정환희","userId":"10718546972386594771"}},"outputId":"747d5aae-aea4-48a6-c17f-90e3210d2dc6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8150833333333334"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[""],"metadata":{"id":"cl6yTGSNW64g"},"execution_count":null,"outputs":[]}]}