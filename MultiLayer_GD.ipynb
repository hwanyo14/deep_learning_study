{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultiLayer_GD.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhbM5SxOYpNO1IpivyODZw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**다층 신경망에 경사 하강법 적용**\n","\n"," - 은닉층: $𝑋𝑊_1 + 𝑏_1 = 𝑍_1 → sig() → 𝐴_1$\n"," - 출력층: $𝐴_1𝑊_2 + 𝑏_2 = 𝑍_2 → sig() → 𝐴_2$\n"," - 예측값: $𝐴_2 → step\\_func() → ŷ$\n","\n","경사 하강법을 적용하기 위해서는 손실함수(로지스틱 손실함수) L의 도함수를 구해야 한다.   \n","<br>\n","##**출력층 가중치에 대하여 손실함수 미분**\n","로지스틱 손실함수 $𝐿$을 출력층 가중치 $𝑊_2$에 대하여 연쇄미분 한다.\n","###$$\n","  \\frac{∂𝐿}{∂𝑊_2} = \\frac{∂𝐿}{∂𝑍_2} \\frac{∂𝑍_2}{∂𝑊_2}\n","$$\n","\n","이전에 구했던 손실함수의 미분값 $-(y - a)x$를 행렬로 표현한 것과 같다.   \n","또한 $x$는 이전 층의 출력인 $𝐴_1$로 생각하면 된다.\n","\n","단, 여기서 주의할 점은 $𝐴_1$은 두 개의 뉴런을 통한 결과값이므로, (m, 2) 크기를 갖는다.   \n","그리고 $-(𝑌 - 𝐴_2)$의 크기는 타깃과 예측값의 차이로, (m, 1) 크기를 갖는다.   \n",">따라서 $𝐴_1$을 전치하여 $-(𝑌 - 𝐴_2)$와 곱해야 한다.\n","\n","###$$\n","  \\frac{∂𝐿}{∂𝑊_2} = \\frac{∂𝐿}{∂𝑍_2} \\frac{∂𝑍_2}{∂𝑊_2}\n","  = 𝐴_1^{𝑇}(-(𝑌 - 𝐴_2))\n","  = \\begin{bmatrix}\n","      -1.37 & … & 2.10 \\\\\n","      0.96 & … & -0.17\n","    \\end{bmatrix}\n","    \\begin{bmatrix}\n","      0.7 \\\\ 0.3 \\\\ ⁝ \\\\ 0.6\n","    \\end{bmatrix}\n","  = \\begin{bmatrix}\n","      -0.12 \\\\ 0.36\n","    \\end{bmatrix}\n","$$\n","\n","두 행렬을 곱한 결과 $𝑊_2$와 같은 (2, 1) 크기의 그레이디언트 행렬을 얻을 수 있다.   \n","현재 구한 그레이디언트 행렬은 모든 샘플에 대한 그레이디언트 총 합이므로,   \n","**가중치 행렬 업데이트를 위해서 평균 그레이디언트를 구해야 한다.**   \n","그 다음 적절한 학습률을 곱하여 가중치 행렬 $𝑊_2$를 업데이트 한다."],"metadata":{"id":"rhveNNl6lTn0"}},{"cell_type":"markdown","source":["##**출력층 절편에 대하여 손실함수 미분**\n","\n","$𝑍_2$를 절편에 대하여 미분하면 $𝟏$(모든 원소가 1인 열벡터)이다.\n","\n","###$$\n","  \\frac{∂𝐿}{∂𝑊_2} = \\frac{∂𝐿}{∂𝑍_2} \\frac{∂𝑍_2}{∂𝑏_2}\n","  = 𝟷^{𝑇}(-(𝑌 - 𝐴_2))\n","  = \\begin{bmatrix}\n","      1 & … & 1\n","    \\end{bmatrix}\n","    \\begin{bmatrix}\n","      0.7 \\\\ 0.3 \\\\ ⁝ \\\\ 0.6\n","    \\end{bmatrix}\n","  = 0.18\n","$$\n","\n","이 값 역시 모든 샘플에 대한 그레이디언트의 합이다.   \n","따라서 전체 샘플의 개수로 나누어 평균 그레이디언트를 구해야 한다.   \n","이로써 출력층에서 그레이디언트를 모두 구했다.   "],"metadata":{"id":"ZDQQ-_IAsYQl"}},{"cell_type":"markdown","source":["##**은닉층 가중치에 대하여 손실함수 미분**\n","\n","은닉층 가중치 $𝑊_1$에 대한 손실함수 미분을 해보자.   \n","###$$\n","  \\frac{∂𝐿}{∂𝑊_1} = \\frac{∂𝐿}{∂𝑍_2} \\frac{∂𝑍_2}{∂𝐴_1}\\frac{∂𝐴_1}{∂𝑍_1}\\frac{∂𝑍_1}{∂𝑊_1}\n","$$\n","꽤 복잡해보이는 식이지만 하나씩 천천히 살표보도록 하자.\n","\n","1. $𝑍_1 = 𝑋𝑊_1 + 𝑏_1$ 이므로 도함수는 $𝑋$ 이다.   \n","예를 들어 입력특성이 3개인 경우 (m, 3) 크기의 행렬을 생각하면 된다.\n","###$$\n","  \\frac{∂𝑍_1}{∂𝑊_1} = 𝑋 = \\begin{bmatrix}\n","                                 3 & 6 & 2 \\\\\n","                                 1 & 10 & 7 \\\\\n","                                 & ⁝ \\\\\n","                                 4 & 8 & 1\n","                              \\end{bmatrix}\n","$$\n","<br>\n","\n","2. 다음은 $\\frac{∂𝐴_1}{∂𝑍_1}$이다.   \n","시그모이드의 도함수가 $a(1 - a)$임을 배웠으므로 $\\frac{∂𝐴_1}{∂𝑍_1}$의 도함수는 $𝐴⊙(1 - 𝐴_1)$ 이다.   \n","원소별 곱셈은 $⊙$ 기호를 사용하고, 도함수의 크기는 (m, 2)가 된다.\n","\n","###$$\n","  \\frac{∂𝐴_1}{∂𝑍_1} = 𝐴⊙(1 - 𝐴_1) = \\begin{bmatrix}\n","                                          2.37 & 6.10 \\\\\n","                                          ⁝ & ⁝ \\\\\n","                                          1.81 & 4.82\n","                                        \\end{bmatrix}\n","$$\n","<br>\n","\n","3. $\\frac{∂𝑍_1}{∂𝐴_1}$는 $𝑍_2 = 𝐴_1𝑊_2 + 𝑏_2$이므로 $𝐴_1$에 대해 미분하면 출력층의 가중치 $𝑊_2$만 남는다.   \n","이 행렬의 크기는 은닉층 뉴런의 개수와 같으므로 (2, 1)이다.\n","###$$\n","  \\frac{∂𝑍_1}{∂𝐴_1} = 𝑊_2 = \\begin{bmatrix} 0.02 \\\\ 0.16 \\end{bmatrix}\n","$$\n","<br>\n","\n","4. 마지막으로 $\\frac{∂𝐿}{∂𝑍_2}$ 이다.\n","###$$\n","  \\frac{∂𝐿}{∂𝑍_2} = -(𝑌 - 𝐴_2) = \\begin{bmatrix} 0.7 \\\\ 0.3 \\\\ ⁝ \\\\ 0.6 \\end{bmatrix}\n","$$\n","\n","이제 이 네 가지의 식을 모두 곱하면 최종적으로 은닝층 가중치에 대한 손실함수 미분식$(\\frac{∂𝐿}{∂𝑊_1})$이 나온다.\n","\n","도함수를 곱하는 순서는 신경망의 역방향으로 진행한다.   \n","가장 먼저 $\\frac{∂𝐿}{∂𝑍_2}$ 와 $\\frac{∂𝑍_1}{∂𝐴_1}$ 를 곱한다.   \n","###$$\n","  \\frac{∂𝐿}{∂𝑍_2}\\frac{∂𝑍_1}{∂𝐴_1} = -(𝑌 - 𝐴_1)𝑊_2^𝑇 = \n","  \\begin{bmatrix} 0.7 \\\\ 0.3 \\\\ ⁝ \\\\ 0.6 \\end{bmatrix}\n","  \\begin{bmatrix} 0.02 & 0.16 \\end{bmatrix}\n","  = \\begin{bmatrix} 0.014 & 0.112 \\\\ ⁝ & ⁝ \\\\ 0.012 & 0.096 \\end{bmatrix}\n","$$"],"metadata":{"id":"XDaaGmRQtf_B"}},{"cell_type":"markdown","source":["###$$\n","  \\frac{∂𝐿}{∂𝑊_1} = \\frac{∂𝐿}{∂𝑍_2} \\frac{∂𝑍_2}{∂𝐴_1}\\frac{∂𝐴_1}{∂𝑍_1}\\frac{∂𝑍_1}{∂𝑊_1}\n","  = 𝑋^𝑇(-(𝑌 - 𝐴_2)𝑊_2^𝑇⊙𝐴_1⊙(1 - 𝐴_1))\n","$$\n","\n","###$$\n","  \\frac{∂𝐿}{∂𝑏_1} = \\frac{∂𝐿}{∂𝑍_2} \\frac{∂𝑍_2}{∂𝐴_1}\\frac{∂𝐴_1}{∂𝑍_1}\\frac{∂𝑍_1}{∂𝑏_1}\n","  = 𝟷^𝑇(-(𝑌 - 𝐴_2)𝑊_2^𝑇⊙𝐴_1⊙(1 - 𝐴_1))\n","$$"],"metadata":{"id":"50xzHoHcbFrN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z164avQNlPLf"},"outputs":[],"source":[""]}]}