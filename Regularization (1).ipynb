{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regularization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPtSdS9w0pEzSAfZUIB9hmX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **규제(Reguarization)**\n","과대적합과 과소적합을 해결하기 위한 방법으로 '규제'가 있다.<br>\n","가중치를 제한하여 모델의 일반화 성능을 올리는 방식이다.<br>\n","규제 방식에는 L1, L2 규제가 있다."],"metadata":{"id":"zhPpeiHrlzsQ"}},{"cell_type":"markdown","source":["#**L1 규제**\n","가중치의 절댓값을 손실함수에 더함<br>\n","**L1 norm = ||w||1 = ∑|w(i)|**\n","\n","***L = -(ylog(a) + (1 - y)log(1 - a)) + α∑|w(i)|***\n","\n","여기서 α는 L1규제의 양을 조절하는 하이퍼파라미터이다.<br>\n","α값이 커지면 전체 손실함수의 값이 커지지 않도록 w값의 합이 작아져야 한다. -> 규제가 강함<br>\n","α값이 작아지면 w값이 커져도 손실 함수의 값이 큰 폭으로 커지지 않는다. -> 규제 약함\n"],"metadata":{"id":"WtSMaMejmmC_"}},{"cell_type":"markdown","source":["경사하강법으로 가중치를 업데이트하기 위해서 L1규제가 적용된 손실함수를 미분해야한다.<br>\n","|w|를 w에 대하여 미분하면 w값의 부호만 남기 때문에 sign(w)로 표현한다.\n","\n","(∂/∂w)L = -(y - a)x + α * sign(w)\n","\n","위의 식에서 학습률도 적용시켜보면,\n","\n","w = w + η((y - a)x - α * sign(w))\n","\n","규제 하이퍼파라미터 α와 가중치 부호를 곱해서 그레디언트에 더한다."],"metadata":{"id":"heexWGQkopNh"}},{"cell_type":"code","source":["w_grad = alpha * np.sign(w)"],"metadata":{"id":"u71Gaa89mkny"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이러한 L1규제를 적용한 회귀 모델을 '라쏘(Lasso)'모델이라고 한다.<br>\n","sklearn.linear_model.Lasso 클래스에서 라쏘모델 제공<br>\n","\n","라쏘모델은 규제 하이퍼파라미터 α에 많이 의존한다.<br>\n","즉, 가중치의 크기에 따라 규제의 양이 변하는 것이 아니므로 규제 효과가 좋지 않다.<br>\n"],"metadata":{"id":"ytnszVLLrNlS"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"tdEAeIWkrKZs"}},{"cell_type":"markdown","source":["# **L2 규제**\n","손실함수에 가중치에 대한 L2 노름의 제곱을 더함.<br>\n","**L2 norm = ||w||2 = √∑|w(i)|²**\n","\n","***L = -(ylog(a) + (1 - y)log(1 - a)) + (1/2)α∑|w(i)|²***<br>\n","(여기서 1/2은 미분결과를 보기 좋게 하기 위함)"],"metadata":{"id":"Y9pkBJ7urMQO"}},{"cell_type":"markdown","source":["L2 규제를 적용한 손실함수를 미분하면 간단히 가중치 벡터 w만 남는다.<br>\n","(∂/∂w)L = -(y - a)x + α * w\n","<br>\n","\n","학습률 하이퍼파라미터까지 적용을 한다면,<br>\n","w = w + η((y - a)x - α * w)<br>\n","\n","그레디언트에 α와 가중치의 곱을 더해준다."],"metadata":{"id":"sJOemiQ3uCk-"}},{"cell_type":"code","source":["w_grad = alpha * w"],"metadata":{"id":"25kYhSRDua0R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["L2 규제는 그레디언트 계산에 가중치 값 자체가 포함되어<br>\n","가중치의 부호만 사용하는 L1보다 효과적이다.<br>\n","따라서 L2 규제를 더 널리 사용한다.\n","<br><br>\n","L2 규제를 적용한 회귀 모델을 릿지 모델이라고 한다.\n","sklearn.linear_model.Ridge 클래스로 제공한다."],"metadata":{"id":"qiw9ZFbfufYs"}}]}